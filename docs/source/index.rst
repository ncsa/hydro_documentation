**NCSA Hydro User Documentation**
============================

**1. Introduction**
--------------

The Hydro cluster combines a current OS and software stack, 256 GB of
memory per node, 40 Gb/s WAN bandwidth, and direct high-performance
access to the Blue Waters home, project, and scratch filesystems,
providing several new capabilities for Blue Waters users:

-  Incorporate software that cannot run on the Cray nodes into
   scientific workflows without the need to move data off of Blue
   Waters.
-  Incorporate calculations that require more than 128 GB of memory per
   node into scientific workflows without the need to move data off of
   Blue Waters.
-  Efficiently import/export data to/from external storage services that
   are not supported by Globus Online.

The Hydro cluster is only intended to support workflow components that
require relatively few node hours but cannot run on the Blue Waters Cray
nodes. Due to the small size of the cluster a reasonable effort should
be made to enable the entire workflow in the Cray environment. Access to
the Hydro cluster may be restricted to NGA-related projects with a clear
need for the resource.

**2. Quick Start Guide to Hydro**
---------------------------------

This information is for users who are adept at using BW and are only
interested in the basic workflow.

**1.** `Getting Access <#Access>`__ - Limited to BW Users who need
access to Hydro

**2.** `Log in to Hydro <#Logging%20in>`__ - example: *ssh hydro*

**3.** `Compile Code <#Compiling>`__ - example: *mpicc -o foo.exe foo.c*

**4.** `Run Code <#Run%20Code>`__ - example: *srun -n 1 ./foo.exe*


.. toctree::
   :maxdepth: 2
   quick_start_guide
   system_description
   accessing_transferring_files
   programming_environments
   partitions_and_job_policies
   running
   support_and_services
   tools_and_utilities


**Software**
~~~~~~~~~~~~

-  RHEL 8.4
-  Kernel 4.18.0
-  Software

   -  Currently software is installed using Spack
   -  A complete list of isntallated software can be generated by the
      command ``module avail`` on the Hydro login node.
   -  A sample of select packages

      -  OpenMPI
      -  FFTW
      -  Python

         -  Tensorflow

      -  GCC 10.2
      -  R
      -  GDAL

**Storage**
~~~~~~~~~~~

-  Hydro users the Blue Waters Lustre file systems. For more information
   see https://bluewaters.ncsa.illinois.edu/storage

**5. Level of Expertise Expected for Blue Waters Hydro Users**
--------------------------------------------------------------

Most users of systems like Blue Waters have experience with other large
high-performance computer systems. The instructions on this portal
generally assume that the reader knows how to use a Unix-style command
line, edit files, run (and modify) Makefiles to build code, write
scripts, and submit jobs to a batch queue system. There *are* some
things that work slightly differently on the Cray XE system than other
systems; the portal documentation covers those in detail, but we assume
that you know the basics already.

If you're not at that level yet (if you're unfamiliar with things like
ssh, emacs, vi, jpico, qsub, make, top) then you'll need to gain some
knowledge before you can use Blue Waters effectively. Here are a few
links to resources that will teach you some of the basics about Unix
command line tools and working on a high-performance computing system:

-  https://www.xsede.org/web/xup/online-training
-  https://newton.utk.edu/bin/view/Main/LinuxCommandLineBasics
-  http://websistent.com/linux-acl-tutorial/ # explains linux Access
   Control Lists (ACL) compared with chmod

**Access and Policy**
=====================

Access to the Hydro cluster is limited to users of allocated NFI
projects and is not a generally allocated resource.

If you are part of an allocated NFI project and would like
access to the Hydro cluster please send email to
`help+hydro@ncsa.illinois.edu <mailto:help+delta@ncsa.illinois.edu?subject=access%20to%20Hydro%20cluster>`__
with a justification for your need to use the cluster.


**Logging In**
--------------

Connect to Hydro via the login hosts at
`hydro.ncsa.illinois.edu <http://bw.ncsa.illinois.edu/>`__ using ssh with
your NCSA DUO passcode or push response from your smartphone (see
instructions below)

-  For help activating your NCSA Duo account, reference `this
   page <https://wiki.ncsa.illinois.edu/display/cybersec/Duo+at+NCSA>`__.
-  To check if your NCSA Duo is working properly, visit
   `here <https://duo.security.ncsa.illinois.edu/portal>`__. Depending
   on the choice you make there, you should receive a pass code or a
   push from Duo.

**Open a command prompt (Run command on Windows):**

-  Once on a Blue Waters login node `link to instructions to get on BW
   login
   node <https://wiki.ncsa.illinois.edu/pages/createpage.action?spaceKey=CRAY&title=link+to+instructions+to+get+on+BW+login+node&linkCreation=true&fromPageId=139133071>`__,
   ssh to hydro

   -  ssh hydro

**Setting Up the Environment**
==============================

**1. Shells and Modules**
-------------------------

The default shell is /bin/bash. You can change it by sending a request
via email to help+bw@ncsa.illinois.edu. (can they not do this through bw
portal as bw LDAP is shared between the two systems?)

The user environment is controlled using the modules environment
management system. Modules may be loaded, unloaded, or swapped either on
a command line or in your $HOME/.bashrc (.cshrc for csh ) shell startup
file.

The command "*module avail \| more"* will display the avail modules on
the system one page at a time.

The module command is a user interface to the Lmod package. The Lmod
package provides for the dynamic modification of the user’s environment
via *modulefiles* (a modulefile contains the information needed to
configure the shell for an application). Modules are independent of the
user’s shell, so both tcsh and bash users can use the same commands to
change the environment.

`Lmod User
Guide <https://lmod.readthedocs.io/en/latest/010_user.html>`__

Useful Module commands:

+----------------------------------+----------------------------------+
| Command                          | Description                      |
+==================================+==================================+
| module avail                     | lists all available modules      |
+----------------------------------+----------------------------------+
| module list                      | lists currently loaded modules   |
+----------------------------------+----------------------------------+
| module help *modulefile*         | help on module modulefile        |
+----------------------------------+----------------------------------+
| module display *modulefile*      | Display information about        |
|                                  | modulefile                       |
+----------------------------------+----------------------------------+
| module load *modulefile*         | load modulefile into current     |
|                                  | shell environment                |
+----------------------------------+----------------------------------+
| module unload *modulefile*       | remove modulefile from current   |
|                                  | shell environment                |
+----------------------------------+----------------------------------+
| module swap *modulefile1         | unload modulefile1 and load      |
| modulefile2*                     | modulefile2                      |
+----------------------------------+----------------------------------+

**To include a particular software stack in your default environment for
hydro login and computes**

Log into hydro login node, manipulate your modulefile stack until
satisfied. *module save;* This will create a .lmod.d/default file. It
will be loaded on hydro login or computes on next login or job
execution.

Useful User Defined Module Collections:

+----------------------------------+----------------------------------+
| Command                          | Description                      |
+==================================+==================================+
| module save                      | Save current modulefile stack to |
|                                  | ~/.lmod.d/default                |
+----------------------------------+----------------------------------+
| module save collection_name      | Save current modulefile stack to |
|                                  | ~/.lmod.d/collectioin_name       |
+----------------------------------+----------------------------------+
| module *restore*                 | Load ~/.lmod.d/default if it     |
|                                  | exists or System default         |
+----------------------------------+----------------------------------+
| module *restore collection_name* | Load your                        |
|                                  | ~/.lmod.d/collectioin_name       |
+----------------------------------+----------------------------------+
| module *reset*                   | Reset your modulefiles to System |
|                                  | default                          |
+----------------------------------+----------------------------------+
| module *disable collection_name* | Disable collection_name by       |
|                                  | adding collection_name~          |
+----------------------------------+----------------------------------+
| module *savelist*                | List all your                    |
|                                  | ~/.lmod.d/collections            |
+----------------------------------+----------------------------------+
| module describe collection_name  | List collection_name modulefiles |
+----------------------------------+----------------------------------+

**2. Home Directory Permissions**
---------------------------------

By default, user home directories and /scratch directories are closed
(permissions 700) with a parent directory setting that prevents users
from opening up the permissions. See the File and Directory Access
Control List page (https://bluewaters.ncsa.illinois.edu/facl) for Blue
Waters file system policies. The /projects file system is designed as
common space for your group; if you want a space that all your group
members can access, that's a good place for it. As always, your space on
the /scratch file system is the best place for job inputs and outputs.

**3. Programming Environment**
------------------------------

The GNU compilers (GCC) version 10.2.0 are in the default user
environment. Version 9.3.0 is also available — load this version with
the command:

+---------------------------------------------+
| ``module load GCC/``\ ``9.3``\ ``.``\ ``0`` |
+---------------------------------------------+

**Compiling**
=============

| To compile MPI code, use the *mpicc, mpiCC, or mpif90* compiler
  wrappers to automatically include the OpenMPI libraries.
| For example:
| *mpicc -o mpi_hello mpi_hello.c*
| If the code also uses OpenMP, include the -fopenmp flag:
| *mpicc -o omp_mpi_hello omp_mpi_hello.c -fopenmp*

**Job Submission**
==================

**1. Running Batch Jobs**
-------------------------

User access to the compute nodes for running jobs is available via a
batch job. Hydro uses the `Slurm Workload
Manager <https://slurm.schedmd.com/overview.html>`__ for running batch
jobs. See the sbatch section for details on batch job submission.

Please be aware that the interactive nodes are a shared resource for all
users of the system and their use should be limited to editing,
compiling and building your programs, and for short non-intensive runs.

**Note:** User processes running on the interactive nodes are killed
automatically if they accrue more than 30 minutes of CPU time or if more
than 4 identical processes owned by the same user are running
concurrently.

An interactive batch job provides a way to get interactive access to a
compute node via a batch job. See the srun or salloc section for
information on how to run an interactive job on the compute nodes. Also,
a very short time *test* queue provides quick turnaround time for
debugging purposes.

To ensure the health of the batch system and scheduler users should
refrain from having more than 1,000 batch jobs in the queues at any one
time.

There is currently 1 partition/queue named normal. The normal
partition's default wallclock time is 4 hours with a limit of 7 days.
Compute nodes are not shared between users.

sbatch
~~~~~~

Batch jobs are submitted through a *job script* using the sbatch
command. Job scripts generally start with a series of SLURM *directives*
that describe requirements of the job such as number of nodes, wall time
required, etc… to the batch system/scheduler (SLURM directives can also
be specified as options on the sbatch command line; command line options
take precedence over those in the script). The rest of the batch script
consists of user commands.

The syntax for sbatch is:

sbatch [list of sbatch options] script_name

The main sbatch options are listed below. Refer to the sbatch man page
for options.

-  | The common resource_names are:
   | --time=\ *time*

   time=maximum wall clock time (d-hh:mm:ss) *[default: maximum limit of
   the queue(partition) submittied to]*

   --nodes=\ *n*

   --ntasks=\ *p* Total number of cores for the batch job

   --ntasks-per-node=\ *p* Number of cores per node (same as ppn under
   PBS)

   | n=number of 16-core nodes *[default: 1 node]*
   | p=how many cores(ntasks) per job or per node(ntasks-per-node) to
     use (1 through 16) *[default: 1 core]*

   | Examples:
   | --time=00:30:00
   | --nodes=2
   | --ntasks=32
   | or
   | --time=00:30:00
   | --nodes=2
   | --ntasks-per-node=16

   **Memory needs:** The compute nodes have 256GB.

   | Example:
   | --time=00:30:00
   | --nodes=2
   | --ntask=32
   | --mem=118000
   | or
   | --time=00:30:00
   | --nodes=2
   | --ntasks-per-node=16
   | --mem-per-cpu=7375

Useful Batch Job Environment Variables

+-----------------+-----------------+-----------------+-----------------+
| Description     | SLURM           | Detail          | | PBS           |
|                 | Environment     | Description     |   Environment   |
|                 | Variable        |                 |   Variable      |
|                 |                 |                 | | *(no longer   |
|                 |                 |                 |   valid)*       |
+=================+=================+=================+=================+
| JobID           | $SLURM_JOB_ID   | Job identifier  | $PBS_JOBID      |
|                 |                 | assigned to the |                 |
|                 |                 | job             |                 |
+-----------------+-----------------+-----------------+-----------------+
| Job Submission  | $S              | By default,     | $PBS_O_WORKDIR  |
| Directory       | LURM_SUBMIT_DIR | jobs start in   |                 |
|                 |                 | the directory   |                 |
|                 |                 | the job was     |                 |
|                 |                 | submitted from. |                 |
|                 |                 | So the cd       |                 |
|                 |                 | $S              |                 |
|                 |                 | LURM_SUBMIT_DIR |                 |
|                 |                 | command is not  |                 |
|                 |                 | needed.         |                 |
+-----------------+-----------------+-----------------+-----------------+
| Machine(node)   | $SLURM_NODELIST | variable name   | $PBS_NODEFILE   |
| list            |                 | that containins |                 |
|                 |                 | the list of     |                 |
|                 |                 | nodes assigned  |                 |
|                 |                 | to the batch    |                 |
|                 |                 | job             |                 |
+-----------------+-----------------+-----------------+-----------------+
| Array JobID     | $SLU            | each member of  | $PBS_ARRAYID    |
|                 | RM_ARRAY_JOB_ID | a job array is  |                 |
|                 | $SLUR           | assigned a      |                 |
|                 | M_ARRAY_TASK_ID | unique          |                 |
|                 |                 | identifier (see |                 |
|                 |                 | the `Job        |                 |
|                 |                 | Arrays <        |                 |
|                 |                 | https://campusc |                 |
|                 |                 | luster.illinois |                 |
|                 |                 | .edu/resources/ |                 |
|                 |                 | docs/user-guide |                 |
|                 |                 | /#jobarrays>`__ |                 |
|                 |                 | section)        |                 |
+-----------------+-----------------+-----------------+-----------------+

Here is a sample Batch script:

:: 
   
   #!/bin/bash
   ### set the wallclock time
   #SBATCH --time=00:30:00

   ### set the number of nodes, tasks per node, and cpus per task for the job
   #SBATCH --nodes=3
   #SBATCH --ntasks-per-node=1
   #SBATCH --cpus-per-task=16

   ### set the job name
   #SBATCH --job-name="hello"

   ### set a file name for the stdout and stderr from the job
   ### the %j parameter will be replaced with the job ID.
   ### By default, stderr and stdout both go to the --output
   ### file, but you can optionally specify a --error file to
   ### keep them separate
   #SBATCH --output=hello.o%j
   ##SBATCH --error=hello.e%j

   ### set email notification
   ##SBATCH --mail-type=BEGIN,END,FAIL
   ##SBATCH --mail-user=username@host

   ### In case of multiple allocations, select which one to charge
   ##SBATCH --account=xyz

   ### For OpenMP jobs, set OMP_NUM_THREADS to the number of
   ### cpus per task for the job step
   export OMP_NUM_THREADS=4

   ## Use srun to run the job on the requested resources. You can change --ntasks-per-node and
   ## --cpus-per-task, as long as --cpus-per-task does not exceed the number requested in the
   ## sbatch parameters
   srun --ntasks=12 --ntasks-per-node=4 --cpus-per-task=4 ./hellope



See the sbatch man page for additional environment variables available.

srun

The srun command initiates an interactive job on the compute nodes.

For example, the following command:

``srun --time=00:30:00 --nodes=1 --ntasks-per-node=16 --pty /bin/bash``

will run an interactive job in the ncsa queue with a wall clock limit of
30 minutes, using one node and 16 cores per node. You can also use other
sbatch options such as those documented above.

After you enter the command, you will have to wait for SLURM to start
the job. As with any job, your interactive job will wait in the queue
until the specified number of nodes is available. If you specify a small
number of nodes for smaller amounts of time, the wait should be shorter
because your job will backfill among larger jobs. You will see something
like this:

``srun: job 123456 queued and waiting for resources``

Once the job starts, you will see:

``srun: job 123456 has been allocated resources``

and will be presented with an interactive shell prompt on the launch
node. At this point, you can use the appropriate command to start your
program.

When you are done with your runs, you can use the exit command to end
the job.

scancel/qdel

The scancel command deletes a queued job or kills a running job.

-  scancel JobID deletes/kills a job.

**2. Job Dependencies**
-----------------------

Job dependencies allow users to set execution order in which their
queued jobs run. Job dependencies are set by using the ??dependency
option with the syntax being ??dependency=<dependency type>:<JobID>.
SLURM places the jobs in *Hold* state until they are eligible to run.

The following are examples on how to specify job dependencies using the
afterany dependency type, which indicates to SLURM that the dependent
job should become eligible to start only after the specified job has
completed.

On the command line:

``sbatch --dependency=afterany:<JobID> jobscript.pbs``

In a job script:

::

   #!/bin/bash
   #SBATCH --time=00:30:00
   #SBATCH --nodes=1
   #SBATCH --ntasks-per-node=16
   #SBATCH --job-name="myjob"
   #SBATCH --output=myjob.o%j
   #SBATCH --dependency=afterany:<JobID>

In a shell script that submits batch jobs:

::

   #!/bin/bash
   JOB_01=`sbatch jobscript1.sbatch |cut -f 4 -d " "`
   JOB_02=`sbatch --dependency=afterany:$JOB_01 jobscript2.sbatch |cut -f 4 -d " "`
   JOB_03=`sbatch --dependency=afterany:$JOB_02 jobscript3.sbatch |cut -f 4 -d " "`
   ...

**Note:** Generally the recommended dependency types to use are after,
afterany, afternotok and afterok. While there are additional dependency
types, those types that work based on batch job error codes may not
behave as expected because of the difference between a batch job error
and application errors. See the dependency section of the sbatch manual
page for additional information (man sbatch).

**3. Job Arrays**
-----------------

If a need arises to submit the same job to the batch system multiple
times, instead of issuing one sbatch command for each individual job,
users can submit a job array. Job arrays allow users to submit multiple
jobs with a single job script using the ??array option to sbatch. An
optional slot limit can be specified to limit the amount of jobs that
can run concurrently in the job array. See the sbatch manual page for
details (man sbatch). The file names for the input, output, etc. can be
varied for each job using the job array index value defined by the SLURM
environment variable SLURM_ARRAY_TASK_ID.

A sample batch script that makes use of job arrays is available in
/projects/consult/slurm/jobarray.sbatch.

**Notes:**

-  | Valid specifications for job arrays are
   | --array 1-10
   | --array 1,2,6-10
   | --array 8
   | --array 1-100%5 (a limit of 5 jobs can run concurrently)

   ::

       

-  You should limit the number of batch jobs in the queues at any one
   time to 1,000 or less. (Each job within a job array is counted as one
   batch job.)

-  Interactive batch jobs are not supported with job array submissions.

-  For job arrays, use of any environment variables relating to the
   JobID (e.g., PBS_JOBID) must be enclosed in double quotes.

-  To delete job arrays, see the
   `qdel <https://campuscluster.illinois.edu/resources/docs/user-guide/#qdel>`__
   command section.

**4. Translating PBS Scripts to Slurm Scripts**
-----------------------------------------------

The following table contains a list of common commands and terms used
with the TORQUE/PBS scheduler, and the corresponding commands and terms
used under the `Slurm scheduler <https://www.msi.umn.edu/slurm>`__. This
sheet can be used to assist in translating your existing PBS scripts
into Slurm scripts to be read by the new scheduler, or as a reference
when creating new Slurm job scripts.

User Commands
~~~~~~~~~~~~~

+----------------------+----------------------+---------------------------------+
| **User Commands**    | **PBS/Torque**       | **Slurm**                       |
+======================+======================+=================================+
| Job submission       | qsub [script_file]   | sbatch [script_file]            |
+----------------------+----------------------+---------------------------------+
| Job deletion         | qdel [job_id]        | scancel [job_id]                |
+----------------------+----------------------+---------------------------------+
| Job status (by job)  | qstat [job_id]       | squeue [job_id]                 |
+----------------------+----------------------+---------------------------------+
| Job status (by user) | qstat -u [user_name] | squeue -u [user_name]           |
+----------------------+----------------------+---------------------------------+
| Job hold             | qhold [job_id]       | scontrol hold [job_id]          |
+----------------------+----------------------+---------------------------------+
| Job release          | qrls [job_id]        | scontrol release [job_id]       |
+----------------------+----------------------+---------------------------------+
| Queue list           | qstat -Q             | squeue                          |
+----------------------+----------------------+---------------------------------+
| Node list            | pbsnodes -l          | sinfo -N OR scontrol show nodes |
+----------------------+----------------------+---------------------------------+
| Cluster status       | qstat -a             | sinfo                           |
+----------------------+----------------------+---------------------------------+

Environment
~~~~~~~~~~~

================ ============== ====================
**Environment**  **PBS/Torque** **Slurm**
================ ============== ====================
Job ID           $PBS_JOBID     $SLURM_JOBID
Submit Directory $PBS_O_WORKDIR $SLURM_SUBMIT_DIR
Submit Host      $PBS_O_HOST    $SLURM_SUBMIT_HOST
Node List        $PBS_NODEFILE  $SLURM_JOB_NODELIST
Q                $PBS_ARRAYID   $SLURM_ARRAY_TASK_ID
================ ============== ====================

Job Specifications
~~~~~~~~~~~~~~~~~~

+----------------------+----------------------+----------------------+
| **Job                | **PBS/Torque**       | **Slurm**            |
| Specification**      |                      |                      |
+======================+======================+======================+
| Script directive     | #PBS                 | #SBATCH              |
+----------------------+----------------------+----------------------+
| Queue/Partition      | -q [name]            | -p [name] **\*Best   |
|                      |                      | to let Slurm pick    |
|                      |                      | the optimal          |
|                      |                      | partition**          |
+----------------------+----------------------+----------------------+
| Node Count           | -l nodes=[count]     | -N [min[-max]]       |
|                      |                      | **\*Autocalculates   |
|                      |                      | this if just task #  |
|                      |                      | is given**           |
+----------------------+----------------------+----------------------+
| Total Task Count     | -l ppn=[count] OR -l | -n OR                |
|                      | mppwidth=[PE_count]  | --ntasks=ntasks      |
+----------------------+----------------------+----------------------+
| Wall Clock Limit     | -l                   | -t [min] OR -t       |
|                      | walltime=[hh:mm:ss]  | [days-hh:mm:ss]      |
+----------------------+----------------------+----------------------+
| Standard Output File | -o [file_name]       | -o [file_name]       |
+----------------------+----------------------+----------------------+
| Standard Error File  | -e [file_name]       | -e [file_name]       |
+----------------------+----------------------+----------------------+
| Combine stdout/err   | -j oe (both to       | (use -o without -e)  |
|                      | stdout) OR -j eo     |                      |
|                      | (both to stderr)     |                      |
+----------------------+----------------------+----------------------+
| Copy Environment     | -V                   | --export=[ALL \|     |
|                      |                      | NONE \| variables]   |
+----------------------+----------------------+----------------------+
| Event Notification   | -m abe               | --mail-type=[events] |
+----------------------+----------------------+----------------------+
| Email Address        | -M [address]         | -                    |
|                      |                      | -mail-user=[address] |
+----------------------+----------------------+----------------------+
| Job Name             | -N [name]            | --job-name=[name]    |
+----------------------+----------------------+----------------------+
| Job Restart          | -r [y \| n]          | --requeue OR         |
|                      |                      | --no-requeue         |
+----------------------+----------------------+----------------------+
| Resource Sharing     | -l                   | --exclusive OR       |
|                      | nac                  | --shared             |
|                      | cesspolicy=singlejob |                      |
+----------------------+----------------------+----------------------+
| Memory Size          | -l mem=[MB]          | --mem=[mem][M \| G   |
|                      |                      | \| T] OR             |
|                      |                      | -                    |
|                      |                      | -mem-per-cpu=[mem][M |
|                      |                      | \| G \| T]           |
+----------------------+----------------------+----------------------+
| Accounts to charge   | -A OR -W             | --account=[account]  |
|                      | group_list=[account] | OR -A                |
+----------------------+----------------------+----------------------+
| Tasks Per Node       | -l mppnppn           | --ta                 |
|                      | [PEs_per_node]       | sks-per-node=[count] |
+----------------------+----------------------+----------------------+
| CPUs Per Task        |                      | --c                  |
|                      |                      | pus-per-task=[count] |
+----------------------+----------------------+----------------------+
| Job Dependency       | -d [job_id]          | --d                  |
|                      |                      | epend=[state:job_id] |
+----------------------+----------------------+----------------------+
| Quality of Service   | -l qos=[name]        | --qos=[normal \|     |
|                      |                      | high]                |
+----------------------+----------------------+----------------------+
| Job Arrays           | -t [array_spec]      | --array=[array_spec] |
+----------------------+----------------------+----------------------+
| Generic Resources    | -l                   | --                   |
|                      | o                    | gres=[resource_spec] |
|                      | ther=[resource_spec] |                      |
+----------------------+----------------------+----------------------+
| Job Enqueue Time     | -a “YYYY-MM-DD       | --begin=YYY          |
|                      | HH:MM:SS”            | Y-MM-DD[THH:MM[:SS]] |
+----------------------+----------------------+----------------------+

**Frequently Asked Questions**
------------------------------

-  Is my Blue Water's allocation charged for Hydro use?

   -  No. There is currently no plan to charge for use of Hydro. (link
      to How is BW different)

-  I see the following when I log in: Lmod has detected the following
   error: The following module(s) are unknown:...

   -  The modules environments are different between Blue Waters and
      Hydro. `See here. <#BWHomedirs>`__

-  If I have an issue, who do I contact?

   -  help+bw@ncsa.illinois.edu
